Comparing the performance of peak callers
------------------------------------------

No research on peak callers and their parameters for ATAC ata has currently
been done. We decided to test two widely used peak callers -- Macs2 (initially
designed for ChIP-Seq data) and FSeq (initially designed for DNaseI data). We
ran peak calling using each of the tools and ranging two parameters which are
expected to have a major influence on peak calling:

- length/width/shifting of the statistical model which directly influences the
  average width of peaks to be called (`-l` for FSeq; `--extsize` for Macs2)
- significance threshold which determines how different from a background noise
  the signal should be to be considered as a peak (`-t` for FSeq; `--qvalue`
  for Macs2)

We used data generated from fresh K562 cells, as we consider it gold standard,
least biased (by freezing/fixing) and most accurate, therefore a perfect
candidate for testing peak callers on.

#### Experimental design

We had three samples (`bam` files as input for Macs2 and `bed` files as input
for FSeq) of fresh K562 cells, namely `fresh1`, `fresh2`, `fresh3`, which were
technical replicates of the same sample.

We ran
[FSeq](https://raw.githubusercontent.com/jknightlab/ATACseq_pipeline/master/Core_manuscript/Fseq_Macs2_Hotspot/run_fseq.sh)
ranging the two parameters in the following way:


- `-l`: 100 200 400 600 800 1000 2000
- `-t`: 2 4 6 8 10 12 14 16 

We ran
[Macs2](https://raw.githubusercontent.com/jknightlab/ATACseq_pipeline/master/Core_manuscript/Fseq_Macs2_Hotspot/run_macs.sh)
ranging the two parameters in the following way:

- `--extsize`: 10 100 200 400 600 800 1000 2000 
- `--qvalue`: 0.001 0.005 0.01 0.05 0.1 0.5 

Code to generate intersections (using Fseq peaks as example):
```
for l in `echo 100 200 400 600 800 1000 2000`
do
    for t in `echo 2 4 6 8 10 12 14 16`
    do
        fresh1="fresh1.fseq.length_$l.tresh_$t.bed"
        fresh2="fresh2.fseq.length_$l.tresh_$t.bed"
        fresh3="fresh3.fseq.length_$l.tresh_$t.bed"
        bedtools intersect -f 0.3 -r -a $fresh1 -b $fresh2 | \
            bedtools intersect -f 0.3 -r -a - -b $fresh3 | \
            bedtools merge -i - > \
            fresh.fseq.length_$l.tresh_$t.intrsct.bed;
    done
done
```

#### Choice of best parameters per peak caller

Choosing best parameters per peak caller turned out to be very complicated. We
looked at things like sensitivity, specificity, F-Score, distribution of called
peaks across annotated categories (TSS, enhancer regions, repressed regions,
etc). The problem with all those metrics is that when we increase the
width/length parameter, peak callers tend to call wider peaks, often join
narrow adjacent peaks. When we have wider window (2000 bases compared to 100),
we will most probably call more "true" events (annotated peaks), simply because
the covered area is so wide. This means that we ideally want maximum number of
"true" (annotated as TSS/Enh/etc) peaks using the minimal number of bases. So
the metric that we took as final to choose the best parameter set per peak
caller is the following:

```
P-Score = PEon * ( BPon/BPtot ),

- P-Score -- **p**erformance estimation/score
- PEon -- number of peaks mapped on target (to areas annotated as TSS/Enh/etc, "desired" areas)
- BPon -- number of bases (sum) in peaks mapped on target
- BPtot -- number of bases (sum) in all called peaks
```

This way we are accounting for the number of "true" peaks that we called, the
number of bases that we called as "true" and the total number of bases (this
account for both off-target peaks -- e.g., in repressed regions -- and peaks
called at annotated regions).

When looking at "true" peaks, we had a variety of options to compare our peaks
to:

- Encode K562 regulatory elements annotation generated by Segwey
- Encode K562 regulatory elements annotation generated by ChromM
- Duke K562 DNaseI sites

To have a most valid set, we only took peaks which were in overlap of all three
data sets (Seqwey+ChromM+DNase).

Further on, we created two peak sets:

- *on target* -- overlap of "1_active_promoter", "2_weak_promoter",
  "4_strong_enhancer", "6_weak_enhancer" from ChromM; "TSS", "Enhancer",
  "Weak_Enhancer" from Segwey; DNaseI peaks;
- *off target* -- overlap of "12_Repressed", "11_Weak_Txn",
  "10_Txn_Elondation", "9_Txn_Transition" from ChromM; "Transcribed",
  "Repressed" from Segwey; DNaseI peaks (overlap with DNaseI peaks might be
  questionable here, as DNase data should not contain those peaks; however, we
  found that after overlapping Segwey and ChromM peaks, further overlapping it
  with DNase peaks does not exclude too many extra peaks).

We required peaks to overlap for at least 25% of their length
(non-reciprocally). This is the code to generate *on target* and *off target*
peak sets:

```
cat wgEncodeAwgSegmentationCombinedK562-T.bed \
    wgEncodeAwgSegmentationCombinedK562-R.bed | \
    bedtools merge -i $i > \
    wgEncodeAwgSegmentationCombinedK562_OFFtarget.bed

bedtools intersect \
    -f 0.25 \
    -a wgEncodeAwgSegmentationCombinedK562_ONtarget.bed \
    -b wgEncodeBroadHmmK562HMM_ONtarget.bed | \
    bedtools intersect \
    -f 0.25 \
    -a wgEncodeAwgDnaseUwdukeK562UniPk.bed \
    -b - > \
    ONtarget.bed 

bedtools intersect \
    -f 0.25 \
    -a wgEncodeAwgSegmentationCombinedK562_OFFtarget.bed \
    -b wgEncodeBroadHmmK562HMM_OFFtarget.bed | \
    bedtools intersect \
    -f 0.25 \
    -a wgEncodeAwgDnaseUwdukeK562UniPk.bed \
    -b - > \
    OFFtarget.bed
```

We used the intersection of peaks called in 3 fresh ATAC K562 samples (peaks
were called intersected if they had at least 30% overlap reciprocally in all
three samples). We required the overlap of 50% of ATAC peak length with an
annotated peak.

Code to generate intersections between peaks and annotation:
```

for i in `ls *intrsct.bed`
do
    name=`echo $i | \
        sed s/100.t/0100.t/g | \
        sed s/200.t/0200.t/g | \
        sed s/400.t/0400.t/g | \
        sed s/600.t/0600.t/g | \
        sed s/800.t/0800.t/g | \
        sed s/h_2.intrsct.bed/h_02.intrsct.bed/g | \
        sed s/h_4.intrsct.bed/h_04.intrsct.bed/g | \
        sed s/h_6.intrsct.bed/h_06.intrsct.bed/g | \
        sed s/h_8.intrsct.bed/h_08.intrsct.bed/g`
    bases=`cat $i | awk '{sum += $3-$2} END {print sum}'`
    on=`bedtools intersect -f 0.5 -a $i -b ../ONtarget.bed | \
        bedtools merge -i - | \
        wc -l`
    on_bp=`bedtools intersect -f 0.5 -a $i -b ../ONtarget.bed | \
        bedtools merge -i - | \
        awk '{sum += $3-$2} END {print sum}'`
    off=`bedtools intersect -f 0.5 -a $i -b ../OFFtarget.bed | \
        bedtools merge -i - | \
        wc -l`
    off_bp=`bedtools intersect -f 0.5 -a $i -b ../OFFtarget.bed | \
        bedtools merge -i - | \
        awk '{sum += $3-$2} END {print sum}'`
    peaks=`cat $i | wc -l`
    echo -e "$name\t$bases\t$peaks\t$on\t$on_bp\t$off\t$off_bp"
done

```
#### Results

We calculated P-Score for all parameter sets tested for F-Seq and Macs2. This
figure demonstrates the results and shows that FSeq performs best with l=200,
t=8; Macs2 performs best with ext=100, q=0.01. These parameter sets are very
different from default parameter sets, and we can see from the heatmaps that
default parameter sets perform far from optimal.

| Peak caller | Default parameters | Chosen parameters   |
| ----------- | ------------------ | ------------------- |
| FSeq        | l=600, t=4         | **l=200, t=8**      |
| Macs2       | ext=200, q=0.05    | **ext=100, q=0.01** |


In this figure, left panel represents the performance of FSeq (as heatmap and
as barplot); right panel represents the performance of Macs2 (as heatmap and as
barplot). Heatmap colours range from blue to violet, blue indicating lower
values and violet indicating higher values. Red square on heatmaps highlights
the best performing parameter sets; blue square indicates the default parameter
set. Barplots show the performance of each parameter set, and red arrow
highlights the best performing parameter set (with the highest performance
score).

![alt text](https://github.com/jknightlab/ATACseq_pipeline/blob/master/Core_manuscript/Fseq_Macs2_Hotspot/macs_fseq_performance.png)


After we chose the best performing parameter set for both FSeq and Macs2, we
compared the two peak lists generated by each peak caller to see which peak
caller performs better. We looked at the following parameters:

- overlap with various categories of the annotation + DNaseI peaks
- peak width distribution
- peak height distribution
- overlap between replicates
- signal-to-noise ratio

**Distribution of peaks across various categories of annotation**

From the two boxplots down below (generated from
[this file](https://github.com/jknightlab/ATACseq_pipeline/blob/master/Core_manuscript/Fseq_Macs2_Hotspot/fseq_macs2_distribution_over_annotation_classes.txt))
we can appreciate that default parameter sets call peaks more peaks in areas
that we do not expect to see ("repressed", "transcribed", "txn"). These
categories disappear for each peak caller when we use the two selected
parameter sets. We can also appreciate that the percentage of peaks falling
within each category is almost identical for both selected parameter sets.


![alt text](https://github.com/jknightlab/ATACseq_pipeline/blob/master/Core_manuscript/Fseq_Macs2_Hotspot/peak_distr_over_annotation.number.png)
![alt text](https://github.com/jknightlab/ATACseq_pipeline/blob/master/Core_manuscript/Fseq_Macs2_Hotspot/peak_distr_over_annotation.percent.png)


**Peak width distribution**

These density plots demonstrate that chosen parameters for both Macs2 and FSeq call peaks with a more narrow width distribution, and this is what we mainly expect to see in ATAC data. We can also appreciate that `FSeq, chosen parameters` has the most narrow width distribution:

![alt text](https://github.com/jknightlab/ATACseq_pipeline/blob/master/Core_manuscript/Fseq_Macs2_Hotspot/peak_width_distribution.png)


**Peak height distribution**



**Overlap between replicates**

From this table we can appreciate that FSeq calls many more peaks than Macs2.
However, a lot of them turn to be non-reproducible (13-25% overlap between
replicates in FSeq compared to 51-54% in Macs2). However, when non-reproducible
peaks are removed, we are left with more peaks when using FSeq for peak
calling. We can appreciate, however, that the amount of peaks overlapping with
annotation (annotated regions such as promoters, enhancers, etc -- but not,
e.g., repressed) is higher for Macs2 (75% for the chosen parameter set) than
for FSeq (62% for the chosen parameter set).

|    | Average number of called peaks | Overlap between replicates | Number of overlapping peaks | Peaks mapped "on target", number | Peaks mapped "on target", percent |
| ------------------------ | ------- | ------ | ------ | ------ | ------ |
| FSeq default parameters  | 221,509 | 13.84% | 30,481 |  7,270 | 23.85% |
| FSeq chosen parameters   |  86,044 | 24.00% | 20,625 | 12,768 | 61.91% |
| Macs2 default parameters |  32,928 | 53.26% | 17,456 |  8,784 | 50.32% |
| Macs2 chosen parameters  |  23,383 | 51.66% | 12,053 |  9,010 | 74.75% |


**Signal to noise ratio**







-------------------------------------------------
developed by Irina Pulyakhina irina@well.ox.ac.uk
